# AI

思维导图： xmind；流程图： ProcessOn/draw.io；组织架构图：Visio/Lucidchart

![AI职业分层](./img/AI职业分层.png)

## AI 模型就是个（大量参数的）函数：输入参数，返回结果

![AI模型的本质](./img/AI模型的本质.png)
![生成策略](./img/生成策略.png)
![文字生成](./img/文字生成.png)
![文字生成2](./img/文字生成2.png)
![文字生成3](./img/文字生成3.png)

生成文字/像素：就像掷骰子一样，随机取一个字/像素；字/像素出现的概率，通过大量（文本）学习排序的

## 大模型应用方向

1. RAG（检索增强生成）：实时接入外部知识库
2. 微调（Fine-tuning）：领域数据适配优化

- 可解释性问题：（严谨领域-医疗）不能只给结果，也要给出为什么？
- 知识的及时性问题：新知识只更新到大模型训练时间；
- 大模型生成的结果与客观事实不相符的问题：答案瞎编乱造；
- 上下文能力的有限性问题：上下文 长度有限；
- 法律约束与伦理问题：
- 成本壁垒问题：
- 其他问题：

## NLP、NLU、NLG

- NLP(自然语言处理-Natural Language Processing)：处理语言形式和结构（如分词）
- NLU(自然语言理解-Natural Language Understanding)：理解语义和上下文
- NLG(自然语言生成-Natural Language Generation)：生成人类语言
- 计算机视觉（CV）：让机器“看懂”图像或视频。

![AI体系分支](./img/AI体系分支.png)
![AI体系分支2](./img/AI体系分支2.png)
![AI体系分支3](./img/AI体系分支3.png)

生成式 AI 目前只能用深度学习实现；未来有可能其他方式实现（目前看不到）

1. 人工智能（AI）：让计算机系统模拟人类智能，从而解决问题和完成任务；

   - 计算机科学下的一个学科；

2. 机器学习（Machine Learning）：核心：不需要人类做显式编程，而是让计算机通过算法，自行学习和改进，去识别模式、做出预测和决策；

   - 是 AI 的一个子集；
   - 示例：给电脑大量玫瑰和向日葵的图片，让电脑自行识别模式、总结规律，从而能能对没见过的图片进行预测和判断；
   - 类型：
     1. 监督学习：用“标注数据”训练模型（如图像分类）。
     2. 无监督学习：从“无标注数据”中发现模式（如聚类）。
     3. 强化学习（RL）：通过“奖励机制”训练智能体（如 AlphaGo）。

3. 深度学习：核心在于使用人工神经网络，模仿人脑处理信息的方式，通过层次化的方法，提取和表述数据特征；

   - 是机器学习的一个方法；
   - 核心：多层神经网络；
   - 典型应用：
     1. GAN（生成对抗网络）
     2. Transformer 架构
     3. LLM（大语言模型）Í

   * 神经网络：有许多基本的计算和存储单元组成；
   * 这些单元被称为神经源；这些神经元通过层层连接起来处理数据，并且深度学习模型通常有很多层（称为深度），每一层都会对数据进行处理和转换，从而提取出更高层次的特征；

- GAI（生成式 AI）：是深度学习的一个应用，能够生成新内容，如文本、图像、视频等；
- LLM（大语言模型）：是深度学习的一个应用，专门用于自然语言处理任务；（特征“大”，GPT、LLaMA）

### 机器学习（Machine Learning）

- 监督学习（Supervised Learning）: 机器学习算法会接收有标签的训练数据，

  1. 标签就是期望的输出值；
  2. 每个训练数据点都包含输入特征和期望的输出值；
  3. 算法的目标是：学习输入和输出之间的映射关系，从而在给“新的输入特征”后能准确预测书相应的输出；

  - 数据集--》监督员打标签（模型训练）--》机器学习算法--》模型
  - 输入测试数据集--》模型--》预测输出

  应用：回归（预测房价）、分类

- 无监督学习：学习的数据是没有标签的；

  1. 算法的任务：自主发现数据里的模式或者规律；

  - AI 算法自动提取特征，自动分类；
  - 原始数据（无标签）--》机器学习算法（寻找规则）--》模型--》归类
  - 可以调整 机器学习算法 按不同规则输出

  应用：聚类（根据颜色/形状分类）、关联规则（推荐算法）

- 强化学习：让模型在环境中采取行动，获得结果反馈，从反馈里学习，从而能在给定情况下采取最佳行动，来最大化奖励或最小化损失；

  1. 机器学习算法自动学习，没有标签，只有奖励和惩罚（采纳和丢弃）；
  2. 逐渐向奖励方向倾斜；
  3. 专注于让智能体（Agent）通过与环境交互学习最优策略，以最大化累积奖励。其核心思想是“试错学习”，类似于人类或动物通过经验改进行为的过程。

  机器学习效果苹果：欠拟合（未达到效果）、最佳拟合、过拟合（没有泛化能力，只能基于训练数据）

![机器学习分类](./img/机器学习.png)
![机器学习分类2](./img/机器学习2.png)

### 深度学习

一种机器学习架构，使用多层人工神经网络，模仿人脑的工作方式来解决复杂的模式识别问题。能够从图像、语音、自然语言中自动提取高层次的特征。

- Transformer：一种基于`自‘注意力’机制`的深度学习模型，用于处理序列数据。它能够并行处理数据，并且能够捕捉长距离的依赖关系，因此在自然语言处理、计算机视觉等领域取得了显著的成果。
  1、从 “片段记忆” 到 “全局记忆”；
  2、从 “串行处理” 到 “高效并行”；

- GPT（Generative Pre-trained Transformer）：生成式预训练 Transformer
- GAI（Generative AI-生成式 AI）：一种能够生成新内容的 AI 技术，如文本生成、图像生成等。

  1. GAI 生成的内容就是 AIGC。
  2. 如 chatGPT、DALL-E 等。

- AIGC（AI-Generated Content）： AI 生成的内容（例文本、图像、视频等）。

- LLM（Large Language Model）：大型语言模型，如 GPT-3、GPT-4 等。

## LLM -- 大型语言模型

1. “大量文本”进行“无监督学习”；借助海量数据，模型能更好的了解单词与上下文之间的关系，从而更好地理解文本的含义，并生成更准确地预测；
2. 参数数量也是巨大；参数是模型内部的变量，可以理解为是模型在训练过程中学到的知识；
   - 参数决定了模型如何对输入数据做出反应，从而决定模型的行为；
   - 例如：做蛋糕时，允许 AI 调整（面粉、糖、蛋）与（面粉、糖、蛋、奶油、牛奶苏打粉、可可粉、时常、温度），可以调整的越多，蛋糕越好吃；
   - GPT-1：1.75 亿参数; GPT-2：15 亿参数; GPT-3：1750 亿参数; GPT-4：1000 万亿参数；
3. 可以完成：生成、分类、总结、改写等工作

![GPT参数量与资料量](./img/GPT参数量与资料量.png)

![大语言模型的四大步骤与资源消](./img/大语言模型的四大步骤与资源消.png)

## 如何训练出一个 AI 聊天助手

1. 通过大量文本进行无监督学习预训练，得到一个能进行文本生成的“基座模型”；
2. 通过一些人类撰写的高质量的对话数据，对基座模型进行监督微调，得到一个“微调后的模型”。此时的模型 除了能续写文本之外，也会具备更好的对话能力；
3. 用“问题和多个对应回答”的数据，让人类标注员对“回答”进行质量排序，然后基于这些数据，训练出一个 `能对回答 进行评分预测的“奖励模型”`；
4. 让第 2 步得到的模型对问题生成回答，用奖励模型对回答进行评分，利用评分作为反馈，进行强化学习训练，就这样 chart GPT 就诞生了；

- 详细

1. 需要海量文本语料库 用来训练；
2. token：大语言模型的基本文本单位；短的英文单词 1 个词 1 个 token，长的英文单词可能被分为多个 token；中文 1 个字可能需要 1 个或者更多 token 来表示；
3. 预训练最烧钱，需要大量数据，大量算力；
4. 基座模型 --微调--》微调后的模型（sft 模型）--（奖励模型）强化--》
5. 奖励模型：提示词--》sft 模型--》生成多个答案，人类对答案进行质量排序，训练出一个奖励模型；

## python

python 代码 --》python 解释器 --》机器；
python 运行过程：翻译一行执行一行；
python 代码 --》编译器 --》机器码；

## Jupyter Notebook 介绍、安装及使用教程

## key : openai / 通义千问（Qwen 阿里云兼容 openai）

1. 密钥保存本地：

   - win: 电脑--》属性--》高级系统设置--》环境变量--》系统变量--》新建：OPENAI_API_KEY:"密钥";
   - mac: 终输入 ps -p $$ ; 查看 CMD 是 bash？还是 zsh？
     1. bash：配置类别位于: ~/.bash_profile “/” zsh：配置类别位于: ~/.zshrc
     1. 打开配置文件，添加：export OPENAI_API_KEY="密钥"
     1. source ~/.zshrc “/” source ~/.bash_profile

2. 代码：

   ```python
   import os
   from openai import OpenAI
   client = OpenAI() #调用 openai直接这样就可以
   #  client = OpenAI(api_key=os.getenv("ALI_API_KEY"),base_url="https://dashscope.ali.com/v1") #调用阿里的通义千问：传 api_key 和 base_url
   response  =client.chat.completions.create(
       model="XXXX", ## 模型版本类型；例如：gpt-3.5-turbo; qwen-1-1-turbo;
       response_format={"type":"json_object"}, ## 部分模型支持：返回格式；例如：json、yaml、xml；
       messages=[
           {
               "role":"user", ## system: 给系统提示；user:用户；assistant:chatGPT的回复；
               "content":"内容",
           }
       ],
       max_tokens=1024, ## 生成内容的最大 token 数量(到达直接截断)；
       temperature=0.9, ## 0-2 之间(默认:1)；改变的是各个token的概率分布；数值越大，越随机，创造性越高；数值越小，越确定，创造性越低；
       top_p=1, ## 0-1 之间(默认:1)，获取词汇表中概率有大到小之和，不超过设定值；数值越大，越随机；数值越小，越确定；
       ## temperature / top_p 二选一调整；
       frequency_penalty=0, ## -2~2 之间(默认:0)，惩罚重复出现的 token；数值越大，惩罚越重（出现过多，降低出现概率）；
       presence_penalty=0, ## -2~2 之间(默认:0)，惩罚不出现的 token；数值越大，惩罚越重（只看是否出现，出现了，就降低概率）；

   )
   print(response.choice[0].message.content)
   ```

3. tiktoken：计算 token 数量；

   ```python
   import tiktoken
   encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
   len(encoding.encode("内容"))  ## 长度就是token个数；
   ```

## 提示词工程

1. 使用最新的模型；
2. 把指令放在提示的开头，并且用###或者“”“来分割指令和上下文；例：“”“文本”“”；
3. 尽可能对上下文和输出的长度、格式、风格等给出具体、描述性、详情的要求；
4. 通过一些例子来阐明想要的输出“格式”；
5. 先从零样本提示开始，效果不好，则用小样本提示；
   - 零样本：不给 AI 任何示范；
   - 小样本：给 AI 一些参考例子；
6. 减少空洞和不严谨的描述；
7. 告诉应该做什么，而不是：不能做什么

- 限定输出格式：
- 小样本提示：给出一些例子，让 AI 按例子回答；、
- 思维链与分步骤思考：一步一步引导 AI 思考；

## LangChain 框架

AI 模型不会存储上次会话内容；

一个强大的 AI 应用不仅是调用模型 api,还能感知上下文，链接外部数据，并且借助外部工具与环境进行互动，来生成更好的回答；因此，LangChain 框架应运而生，它是一个开源的框架，用于构建和操作语言模型应用程序；它提供了一套工具和库，用于构建和管理语言模型应用程序，包括数据输入、模型调用、结果输出等环节，并且支持多种语言模型，包括 GPT-3、GPT-4 等；

- LangChain 提供了一系列组件和链来简化开发过程；

  例：如果希望 AI 有记忆，借助 LangChain 不需要手动维护一个储存消息的列表，而是创建一个 ConversationBufferMemory 实力，连同模型的实例一起作为对话链的参数，之后每次对话时，新消息都会自动被添加，全部传给模型，相当于模型被外接了记忆，帮助应用开发者管理对话状态；

- LangChain 为“不同模型”提供了统一的接口（抽象层）；
- ChatModal(聊天模型)

- Assistant API 与 LangChain 区别：

  1. Assistant API：通过 Assistant API 发送文本提示给模型，然后接受模型生成的回应；
  2. LangChain：利用 LangChain 提供的多种工具和组件，创建基于模型的应用；
  3. 支持的范围不同：
     - Assistant API：仅支持 openAI 模型；
     - LangChain：支持多种模型，包括 openAI、文心一言、通义千问 等；

- LangChain 的核心组件：

  1. 模型（Model）：语言模型，提供语言理解和生成能力，是 AI 应用的核心；（如 GPT-3、GPT-4、通义千问 等；）
  2. 记忆（Memory）：用来储存和管理对话历史（相关的上下文信息）；这个组件是对话型 AI 应用中保持连贯性和上下文感知的关键
  3. 链（Chain）：是把不同组件串联起来的结构，能让我们创建出复杂的流程，流程里的每个组件可以负责处理特定的任务
  4. 检索器（Retriever）：负责从外部信息源检索信息，对增加模型的知识面和回答准确性很重要
  5. agent（智能体/代理）：代表一个基于大模型的，能执行一系列动作的智能体；核心理念是利用 AI 模型的能力进行推理，根据任务，动态评估和确定行动路径

## RAG（Retrieval Augmented Generation - 检索增强生成）

应用空间：小众细分领域，例：公司内部数据、个人私密文件等；

检索增强生成：提供外部文档，让模型访问外部知识库，获得实时且正确的数据，生成更可靠和准确的回答；当用户提出和外部知识相关的问题后，AI 可以结合知识库里的内容，进行回答；

1. 准备外部数据；外部文档要先加载出来，并且切分成一个个文本块（因为大语言模型的上下文窗口有限，即一次能接收的文本长度有限），然后每个文本块会被转成一系列的向量（可以把向量看作一串固定长度的数字），文本块并不能随便转成数字，向量中要包含文本之间的语法语义等关系（例：相似的文本所对应的嵌入向量，在向量空间里的距离更近，而一些没关系的文本之间的距离就更远）；这有助于模型基于数学，计算向量空间里的距离，去捕捉不同文本在语义和语法等方面的相似； 这些向量都要被储存进向量数据库里，现在外部数据就准备好了；
2. 搜索；当用户提出问题时，这个提示也会被转换为向量，然后查找向量数据库里和用户的查询向量距离最近的段落向量（距离近就表示他们内容相似）；
3. 询问；上一步中和用户查询最为接近的段落被提取了出来，于是，这个段落会和用户的查询组合到一起，一块传给 AI 模型，这样 AI 就能把外部文档的段落作为上下文，基于里面的信息，返回更准确的回答；
4. 因此，借助 RAG，用户可以对外部文档里任何内容进行提问，即使 AI 模型从来没有受到过那些内容的训练； RAG 有利于搭建企业知识库或个人知识库

## ReAct（Reason and Action - 推理和行动）

AI 局限性：大模型天然受到训练数据日期的影响 - 知识截断；

模型持续更新：ReAct（推理和行动） 核心：让模型进行“动态推理”，并采取行动与外界环境互动；也就是我们会引导模型进行推理，并且让它知道可以根据外界环境采取哪些行动。

为了让模型实现 ReAct，我们可以借助思维链，用小样本提示展示给模型一个推理与行动结合框架；也就是针对问题 把步骤进行拆分，每个步骤要经过 推理、行动、观察；
推理：是针对问题或上一步观察的思考；
行动：是基于推理与外部环境的一些交互；例用搜索引擎对关键字进行搜索；
观察：是对行动得到的结果进行查看；

推理和行动：AI 模型可以基于当前状态，推理出下一步的行动，并执行这个行动；

## agent（智能体） 执行器

AI 既能根据用户的输入以及环境进行动态推理，也能基于推理采取合理的行动，并且在需要的时候借助合适的外部工具，通过结合不同的工具来增强模型的功能和效率，我们把这个能理解用户的查询或指令进行推理并执行特定任务，最后输出响应的服务叫做 agent（智能体或代理）

![agent-1-推理](./img/agent-1-推理.png)
![agent-2-行动](./img/agent-2-行动.png)
![agent-3-观察](./img/agent-3-观察.png)
![agent-4-循环](./img/agent-4-循环.png)

## PAL

## 应用

- R1： 复杂问题推理、数学/代码求解（学术研究、技术分析、代码开发）
- v3： 多轮对话、指令理解、任务执行（客服、日常对话、内容生成）

- vscode + cline 插件 + 硅基流动

###

- AI 技术栈全景图：

  基础设施层 --》模型层 --》应用层

  1. 基础设施层：PyTorch / SiliconFlow --》模型训练框架；
  2. 模型层：DeepSeek / GPT（openAI） --》模型服务（LLM 服务）；
  3. 应用层：LangChain （RAG / ReAct / PAL） --》应用开发（智能应用）；

- 应用开发层

  | 组件             | 功能         | 典型用例          | 关键技术           |
  | :--------------- | :----------- | :---------------- | :----------------- |
  | LangChain-Memory | 对话状态管理 | 多轮客服会话      | ConversationBuffer |
  | LangChain-RAG    | 知识库增强   | 企业文档问答      | 向量数据库(Chroma) |
  | LangChain-Agent  | 工具调用     | 实时数据查询+分析 | ReAct 框架         |

- RAG vs 微调

  | 维度     | RAG（检索增强生成） | 微调(Fine-tuning) |
  | :------- | :------------------ | :---------------- |
  | 适用场景 | 知识实时更新        | 领域术语/风格适应 |
  | 成本     | 低（无需重训练）    | 高（需 GPU 资源） |
  | 延迟     | 较高（需检索）      | 低（直接推理）    |

- 开发实践指南 -- LangChain 集成方案： 用户输入-->是否需要知识库

  1. 需要：RAG 流程-->向量检索-->上下文拼接
  2. 不需要：直接调用 LLM-->生成响应

###

- 效率工具

  - Token 计算：tiktoken 库精准控制成本
  - 本地调试：Ollama+ChatBox 本地测试方案
    1. Ollama 是一个开源的本地大语言模型运行框架，专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计
    2. 客户端：个人【chatbox/Cherry-Studio】；公司网页【Open-WebUI】
  - 提示工程：结构化指令+示例引导（Few-shot）

- 常见组合用例

1. 用 PyTorch 或 硅基流动（框架） 训练模型 → 用 DeepSeek（模型） 的 API 调用大模型 → 通过 LangChain 集成到聊天机器人中。
   PyTorch/SiliconFlow --训练模型-->DeepSeek --提供模型能力-->LangChain--构建应用-->终端产品

   - PyTorch/SiliconFlow 是底层框架；LangChain 是上层工具；它们均可服务于 OpenAI 或 DeepSeek 的模型应用。
   - 地缘因素（如国产化需求）可能推动 DeepSeek + SiliconFlow 的组合替代 OpenAI + PyTorch。

   * 模型开发层：使用 PyTorch/SiliconFlow 训练模型（如 DeepSeek 的基座模型）。
   * 模型服务层：DeepSeek 将训练好的大模型封装为服务（开源或 API）。
   * 应用构建层：LangChain 集成 DeepSeek 的模型，并添加业务逻辑（如连接客户数据库）。

2. 最佳实践路径

   1.数据准备 → 2. 基座模型选择 → 3. RAG/微调 → 4. LangChain 集成 → 5. Agent 部署

3. 为什么需要三者协作？
   - PyTorch/SiliconFlow 解决“如何造模型”的问题。
   - DeepSeek 解决“用什么模型”的问题（避免从零训练）。
   - LangChain 解决“如何用模型”的问题（快速落地应用）。
